{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/media/senilab/DATA/Master/I3D-Tensorflow/')\n",
    "import os\n",
    "import time\n",
    "import numpy\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "import input_test\n",
    "import math\n",
    "import numpy as np\n",
    "# from i3dCopy1 import InceptionI3d\n",
    "from i3d import InceptionI3d\n",
    "from utils import *\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import hickle\n",
    "from os import listdir \n",
    "from os.path import isfile, join, isdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multihead_attention(queries, \n",
    "                        keys, \n",
    "                        num_units=None, \n",
    "                        num_heads=64, \n",
    "                        dropout_rate=0,\n",
    "                        is_training=True,\n",
    "                        causality=False,\n",
    "                        scope=\"multihead_attention\", \n",
    "                        reuse=None):\n",
    "    \n",
    "\n",
    "    '''Applies multihead attention.\n",
    "    \n",
    "    Args:\n",
    "      queries: A 3d tensor with shape of [N, T_q, C_q].\n",
    "      keys: A 3d tensor with shape of [N, T_k, C_k].\n",
    "      num_units: A scalar. Attention size.\n",
    "      dropout_rate: A floating point number.\n",
    "      is_training: Boolean. Controller of mechanism for dropout.\n",
    "      causality: Boolean. If true, units that reference the future are masked. \n",
    "      num_heads: An int. Number of heads.\n",
    "      scope: Optional scope for `variable_scope`.\n",
    "      reuse: Boolean, whether to reuse the weights of a previous layer\n",
    "        by the same name.\n",
    "        \n",
    "    Returns\n",
    "      A 3d tensor with shape of (N, T_q, C)  \n",
    "    '''\n",
    "    with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n",
    "        # Set the fall back option for num_units\n",
    "        if num_units is None:\n",
    "            num_units = queries.get_shape().as_list[-1]\n",
    "        \n",
    "        # Linear projections\n",
    "        Q = tf.contrib.layers.fully_connected(queries, num_units) # (N, T_q, C)\n",
    "        K = tf.contrib.layers.fully_connected(queries, num_units ) # (N, T_k, C)\n",
    "        V = tf.contrib.layers.fully_connected(keys, num_units) # (N, T_k, C)\n",
    "        \n",
    "        Q1 = tf.reshape(Q,(Q.get_shape().as_list()[0],Q.get_shape().as_list()[1],num_units))\n",
    "        K1 = tf.reshape(K,(Q.get_shape().as_list()[0],Q.get_shape().as_list()[1],num_units))        \n",
    "        V1 = tf.reshape(V,(Q.get_shape().as_list()[0],Q.get_shape().as_list()[1],num_units))\n",
    "        \n",
    "        \n",
    "        # Split abboxfnd concat\n",
    "        Q_ = tf.concat(tf.split(Q1, num_heads, axis = 2),axis =0) # (h*N, T_q, C/h) \n",
    "        K_ = tf.concat(tf.split(K1, num_heads, axis = 2),axis =0) # (h*N, T_k, C/h) \n",
    "        V_ = tf.concat(tf.split(V1, num_heads, axis = 2),axis =0) # (h*N, T_k, C/h) \n",
    "\n",
    "        # Multiplication\n",
    "        outputs = tf.matmul(Q_, tf.transpose(K_, [0, 2, 1])) # (h*N, T_q, T_k)\n",
    "        \n",
    "        # Scale\n",
    "        outputs = outputs / (K_.get_shape().as_list()[-1] ** 0.5)\n",
    "\n",
    "                \n",
    "        # Key Masking\n",
    "        key_masks = tf.sign(tf.abs(tf.reduce_sum(keys, axis=-1))) # (N, T_k)\n",
    "        key_masks = tf.tile(key_masks, [num_heads, 1]) # (h*N, T_k)\n",
    "        key_masks = tf.tile(tf.expand_dims(key_masks, 1), [1, tf.shape(queries)[1], 1]) # (h*N, T_q, T_k)\n",
    "       # \n",
    "        paddings = tf.ones_like(outputs)*(-2**32+1)\n",
    "        outputs = tf.where(tf.equal(key_masks, 0), paddings, outputs) # (h*N, T_q, T_k)\n",
    "          \n",
    "        # Causality = Future blinding\n",
    "        if causality:\n",
    "            diag_vals = tf.ones_like(outputs[0, :, :]) # (T_q, T_k)\n",
    "            tril = tf.linalg.LinearOperatorLowerTriangular(diag_vals).to_dense() # (T_q, T_k)\n",
    "            masks = tf.tile(tf.expand_dims(tril, 0), [tf.shape(outputs)[0], 1, 1]) # (h*N, T_q, T_k)\n",
    "  # \n",
    "            paddings = tf.ones_like(masks)*(-2**32+1)\n",
    "            outputs = tf.where(tf.equal(masks, 0), paddings, outputs) # (h*N, T_q, T_k)\n",
    "  \n",
    "        # Activation\n",
    "        outputs = tf.nn.softmax(outputs) # (h*N, T_q, T_k)\n",
    "        matt    = outputs\n",
    "        \n",
    "        \n",
    "        # Query Masking\n",
    "        query_masks = tf.sign(tf.abs(tf.reduce_sum(queries, axis=-1))) # (N, T_q)\n",
    "        query_masks = tf.tile(query_masks, [num_heads, 1]) # (h*N, T_q)\n",
    "        query_masks = tf.tile(tf.expand_dims(query_masks, -1), [1, 1, tf.shape(keys)[1]]) # (h*N, T_q, T_k)\n",
    "        outputs *= query_masks # broadcasting. (N, T_q, C)\n",
    "          \n",
    "        # Dropouts\n",
    "        outputs = tf.contrib.layers.dropout(outputs, keep_prob=dropout_rate, is_training=tf.convert_to_tensor(is_training))\n",
    "               \n",
    "        # Weighted sum\n",
    "        outputs = tf.matmul(outputs, V_) # ( h*N, T_q, C/h)\n",
    "        \n",
    "        # Restore shape\n",
    "        outputs = tf.concat(tf.split(outputs, num_heads,axis = 0),axis =2 ) # (N, T_q, C)\n",
    "              \n",
    "        # Residual connection\n",
    "        outputs += queries              \n",
    "              \n",
    "        # Normalize\n",
    "        outputs = normalize(outputs) # (N, T_q, C)\n",
    " \n",
    "    return outputs, matt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(inputs, \n",
    "              epsilon = 1e-8,\n",
    "              scope=\"ln\",\n",
    "              reuse=None):\n",
    "    '''Applies layer normalization.\n",
    "    \n",
    "    Args:\n",
    "      inputs: A tensor with 2 or more dimensions, where the first dimension has\n",
    "        `batch_size`.\n",
    "      epsilon: A floating number. A very small number for preventing ZeroDivision Error.\n",
    "      scope: Optional scope for `variable_scope`.\n",
    "      reuse: Boolean, whether to reuse the weights of a previous layer\n",
    "        by the same name.\n",
    "      \n",
    "    Returns:\n",
    "      A tensor with the same shape and data dtype as `inputs`.\n",
    "    '''\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        inputs_shape = inputs.get_shape()\n",
    "        params_shape = inputs_shape[-1:]\n",
    "    \n",
    "        mean, variance = tf.nn.moments(inputs, [-1], keep_dims=True)\n",
    "        beta= tf.Variable(tf.zeros(params_shape))\n",
    "        gamma = tf.Variable(tf.ones(params_shape))\n",
    "        normalized = (inputs - mean) / ( (variance + epsilon) ** (.5) )\n",
    "        outputs = gamma * normalized + beta\n",
    "        \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(inputs,\n",
    "                        maxlen,\n",
    "                        masking=True,\n",
    "                        scope=\"positional_encoding\"):\n",
    "    '''Sinusoidal Positional_Encoding. See 3.5\n",
    "    inputs: 3d tensor. (N, T, E)\n",
    "    maxlen: scalar. Must be >= T\n",
    "    masking: Boolean. If True, padding positions are set to zeros.\n",
    "    scope: Optional scope for `variable_scope`.\n",
    "    returns\n",
    "    3d tensor that has the same shape as inputs.\n",
    "    '''\n",
    "\n",
    "    E = inputs.get_shape().as_list()[-1] # static\n",
    "    N, T = tf.shape(inputs)[0], tf.shape(inputs)[1] # dynamic\n",
    "    with tf.variable_scope(scope, reuse=True):\n",
    "        # position indices\n",
    "        position_ind = tf.tile(tf.expand_dims(tf.range(T), 0), [N, 1]) # (N, T)\n",
    "\n",
    "        # First part of the PE function: sin and cos argument\n",
    "        position_enc = np.array([\n",
    "            [pos / np.power(10000, (i-i%2)/E) for i in range(E)]\n",
    "            for pos in range(maxlen)])\n",
    "\n",
    "        # Second part, apply the cosine to even columns and sin to odds.\n",
    "        position_enc[:, 0::2] = np.sin(position_enc[:, 0::2])  # dim 2i\n",
    "        position_enc[:, 1::2] = np.cos(position_enc[:, 1::2])  # dim 2i+1\n",
    "        position_enc = tf.convert_to_tensor(position_enc, tf.float32) # (maxlen, E)\n",
    "\n",
    "        # lookup\n",
    "        outputs = tf.nn.embedding_lookup(position_enc, position_ind)\n",
    "\n",
    "        # masks\n",
    "        if masking:\n",
    "            outputs = tf.where(tf.equal(inputs, 0), inputs, outputs)\n",
    "\n",
    "        return tf.to_float(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datasavetry = '/media/senilab/DATA2/array_hmdb/train_rgb/Veoh_Alpha_Dog_1_walk_f_cm_np1_ba_med_7'\n",
    "add = hickle.load(datasavetry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir_rgb = '/media/senilab/DATA2/array_hmdb/train_rgb/'\n",
    "train_dir_flow = '/media/senilab/DATA2/array_hmdb/train_flow/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_train_rgb = sorted([f for f in listdir(train_dir_rgb) if isfile(join(train_dir_rgb, f))])\n",
    "list_train_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list_train_rgb:\n",
    "    data_file = train_dir_rgb + i\n",
    "    data = hickle.load(data_file)\n",
    "    n_frames = 64\n",
    "    n_hidden = 512\n",
    "\n",
    "    group1 = data['feat']\n",
    "    position_idx  = tf.ones((n_frames,1*n_hidden))\n",
    "    positional_encode_tmp = positional_encoding(tf.expand_dims(position_idx, axis=0), n_frames) \n",
    "    group1 = group1 + positional_encode_tmp\n",
    "\n",
    "    multi_g1, matt1  = multihead_attention(queries=group1, keys=group1, num_units=n_hidden, num_heads=64,dropout_rate=0.9,is_training=1,causality=True, scope = 'forward', reuse = tf.AUTO_REUSE)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_frames = 64\n",
    "n_hidden = 512\n",
    "\n",
    "label = data['label']\n",
    "group1 = data['feat']\n",
    "position_idx  = tf.ones((n_frames,1*n_hidden))\n",
    "positional_encode_tmp = positional_encoding(tf.expand_dims(position_idx, axis=0), n_frames) \n",
    "group1 = group1 + positional_encode_tmp\n",
    "\n",
    "multi_g1, matt1  = multihead_attention(queries=group1, keys=group1, num_units=n_hidden, num_heads=64,dropout_rate=0.9,is_training=1,causality=True, scope = 'forward', reuse = tf.AUTO_REUSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matt2 = tf.concat([multi_g1, multi_g1], 0) \n",
    "matt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# WandB â€“ Config is a variable that holds and saves hyperparameters and inputs\n",
    "    # Initialize config\n",
    "batch_size = 4         # Input batch size for training\n",
    "learning_rate = 0.0001    # Learning rate\n",
    "num_steps = 25000       # Number of batches to train\n",
    "display_step = 100      # Steps between displaying output\n",
    "num_input = 64         #(img shape: 64 * 64 * 64 = 262144)\n",
    "num_classes = 51        #(0-9 categories)\n",
    "n_hidden_1 = 512\n",
    "n_hidden_2 = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([num_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, num_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.5\n",
    "epochs = 10\n",
    "batch_size = 100\n",
    "\n",
    "# declare the training data placeholders\n",
    "# input x - for 28 x 28 pixels = 784\n",
    "x = tf.placeholder(tf.float32, [None, 64])\n",
    "# now declare the output data placeholder - 10 digits\n",
    "y = tf.placeholder(tf.float32, [None, 51])\n",
    "\n",
    "\n",
    "# now declare the weights connecting the input to the hidden layer\n",
    "W1 = tf.Variable(tf.random_normal([64, 512], stddev=0.03), name='W1')\n",
    "b1 = tf.Variable(tf.random_normal([300]), name='b1')\n",
    "# and the weights connecting the hidden layer to the output layer\n",
    "W2 = tf.Variable(tf.random_normal([300, 10], stddev=0.03), name='W2')\n",
    "b2 = tf.Variable(tf.random_normal([10]), name='b2')\n",
    "\n",
    "hidden_out = tf.add(tf.matmul(multi_g1, W1), b1)\n",
    "hidden_out = tf.nn.relu(hidden_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "def neural_net(x):\n",
    "    \n",
    "    # Hidden fully connected layer with 128 neurons\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "#     layer_1 = tf.add(tf.tensordot(tf.cast(x, tf.float32), weights['h1'], [[2], [0]]), biases['b1'])\n",
    "    \n",
    "    # Hidden fully connected layer with 128 neurons\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    \n",
    "    # Output fully connected layer with a neuron for each class\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    \n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hasil = neural_net(multi_g1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
